[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning is messy, performance is polished",
    "section": "",
    "text": "Preface\nThis book is a memoir and a manifesto. The content comes from Laura Pankratz’ many years of experience as an educator, and the text is written by Laura’s daughter, Dr. Elizabeth Pankratz.\nWe see learning as a cognitive process with behavioural outcomes. This book is built around these key ideas.\n\nPart 1: What is learning?\nPart 2: The cognitive process of learning, and how to facilitate it\nPart 3: The behavioural outcomes of learning, and how to assess them\n\n\n\nBios\n\n\n\n\n\nLaura Pankratz is an educator with more than 30 years of working with students and teachers. One of her principles of education is that learning is messy. Her passion is finding the nugget of value in that mess and then helping the learner to build around it. Laura’s career included teaching grades nine to 12, working with Alberta Education setting the Physics 30 Diploma Examination, two years as Teacher-in-Residence at the Perimeter Institute for Theoretical Physics, and many presentations at science councils across Canada and around the world. She looks forward to learning with participants at every opportunity.\n\n\n\n\n\nDr. Elizabeth Pankratz (personal website) is also an educator, a Lecturer (assistant professor) at the University of Edinburgh. Her work focuses on helping Psychology students build their confidence and skills in statistical analysis. Her favourite moments are when a student overcomes their wariness of statistics and discovers that they’re capable of more than they thought.\n\n\n\n\nLaura’s acknowledgements\nMy ideas have formed as a result of conversations with excellent educators, workshops I have attended or presented, and years in high school classrooms or in assessment.\nListing names means someone won’t get mentioned who should, and if that is you please extend grace!\nI have deep and abiding appreciation for Jack, Steve, Jeff, Dion, Kristian, Erin, Jason, Shane, Stan, Shannon, Bill, Dave 1, Dave 2, and Lawrence.\nFor the dear friends who have listened to endless rants about education or who have engaged in their own rants, much love to Al and Kristina.\nSeparate mention of my two years as Teacher-in-Residence and the more than 10 years as a teacher network regional coordinator for the Perimeter Institute for Theoretical Physics is necessary. The opportunity to work with teachers across Canada and around the world who are passionate about student learning provided me with energy and an outlet for my needs to learn and to help others learn. The people here include Greg, Dave, Kelly, Damian, Sean, Saara, Simon, Nanouk, and Philip.\nThank you, ALL, for your years of support, friendship, and professionally challenging conversations.\n\n\nTo cite this work\nAPA 7:\n\nPankratz, L., & Pankratz, E. (2025). Learning is messy, performance is polished. [Online book]. https://elizabethpankratz.github.io/learning-is-messy/\n\nBibTeX:\n@book{PankratzPankratz2025,\n  title = {Learning is messy, performance is polished},\n  author = {Pankratz, Laura and Pankratz, Elizabeth},\n  year = {2025},\n  publisher = {[Online book]}\n}",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "part1/intro-learning.html",
    "href": "part1/intro-learning.html",
    "title": "1  Defining learning",
    "section": "",
    "text": "Learning is a process, a change in cognition. As educators, we try to facilitate this cognitive process (our focus in Part 2 of this book). But we can’t see into learners’ minds to know what has actually changed. All we can access is what learners do with what they’ve learned. Based on their behaviour, we make inferences about their learning (our focus in Part 3 of this book).\nIn very broad terms, learning can be defined as the process of building a consistent response to a stimulus. You know you’ve learned when you respond to a stimulus in a new and consistent way. And different levels of learners may show different kinds of consistent responses.\n\nA novice learner will have a memorised response to a stimulus. This response is consistent, but it’s not nuanced nor connected to other ideas. When faced with a new context, the novice learner might say something like, “You’ve not taught me how to answer this,” or “I don’t know where to begin.”\nAn intermediate learner knows that there are nuances behind an idea and connections to be made, but might not be able to convey them precisely. When faced with a new context, the intermediate learner might give responses at a simpler level of cognition than what the new context demands (we’ll talk more about levels of cognition next, in Section 2), or they might say something like, “I’ve memorised everything, but I still don’t understand!”\nA master learner knows the nuances and connections and chooses the response that’s most appropriate in each situation. When faced with a new context, a master learner recognises what parts of their understanding are transferrable and is able to situate them in the new scenario.\n\nEven though these responses all reflect different degrees or levels of cognition, they all still reflect learning. Next we’ll discuss two common ways of conceptualising these different levels of cognition: Bloom’s taxonomy and the SOLO taxonomy.",
    "crumbs": [
      "What is learning?",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Defining learning</span>"
    ]
  },
  {
    "objectID": "part1/learning-taxonomies.html",
    "href": "part1/learning-taxonomies.html",
    "title": "2  Taxonomies of learning",
    "section": "",
    "text": "2.1 Bloom’s taxonomy\nProbably the most widespread framework for classifying levels of cognition in learning is Bloom’s taxonomy (Bloom et al., 1956), more recently refined by Anderson & Krathwohl (2001). Anderson and Krathwohl frame six levels of cognition as actions, here ordered from easiest to most demanding:\nTo use Bloom’s taxonomy, educators need to interpret these six levels and map them to what they infer a student is actually doing when the student generates a response.\nIn Laura’s experience, different educators interpret these levels differently, leading to people using the same words for different concepts (and different words for the same concepts). For this reason, she prefers the SOLO taxonomy. It is easier to explain, to internalise, and to use.",
    "crumbs": [
      "What is learning?",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Taxonomies of learning</span>"
    ]
  },
  {
    "objectID": "part1/learning-taxonomies.html#the-solo-taxonomy",
    "href": "part1/learning-taxonomies.html#the-solo-taxonomy",
    "title": "2  Taxonomies of learning",
    "section": "2.2 The SOLO taxonomy",
    "text": "2.2 The SOLO taxonomy\nSOLO stands for “Structure of the Observed Learning Outcome”. This taxonomy was developed by Biggs & Collis (1982), and it specifies five levels of increasingly complex thinking and understanding:\n\nPre-structural: The learner hasn’t started learning.\nUni-structural: The learner can describe an idea or do a single-step procedure.\nMulti-structural: The learner has a grasp on several distinct ideas but doesn’t know how they fit together.\nRelational: The learner can link, classify, sequence, compare, and contrast their ideas.\nExtended abstract: The learner can generalise from their ideas and make predictions about new contexts.\n\nSOLO’s major strength is that it describes relationships among ideas, rather than Bloom’s approach of classifying actions. And relationships among ideas are crucial for our work as educators: we must always be aware of the way students are linking (or not yet linking) their ideas.\nWe’ll come back to these levels of cognition again and again throughout this book, because Laura uses the SOLO taxonomy in many different areas of her teaching. For example:\n\nto interpret the cognitive level of questions from students,\nto decide on the appropriate level of answer to provide,\nto evaluate the level of a student’s answer compared to the level expected by the question,\nto choose the scope of a task, and\nto determine expectations of an assessment or a course.\n\nThe web site Inspiring Inquiry is a very good resource for exploring the SOLO taxonomy in greater depth.\n\n\n\n\n\n\n\n\n\nAnderson, L. W., & Krathwohl, D. R. (2001). A taxonomy for learning, teaching, and assessing: A revision of Bloom’s taxonomy of educational objectives. Longman.\n\n\nBiggs, J. B., & Collis, K. F. (1982). Evaluating the quality of learning: The SOLO taxonomy (Structure of the Observed Learning Outcome). Academic Press, Inc.\n\n\nBloom, B., Engelhart, M. D., Furst, E. J., Hill, W. H., & Krathwohl, D. R. (1956). Taxonomy of educational objectives: The classification of educational goals. Longman.",
    "crumbs": [
      "What is learning?",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Taxonomies of learning</span>"
    ]
  },
  {
    "objectID": "part2/intro-cognitive.html",
    "href": "part2/intro-cognitive.html",
    "title": "The cognitive process of learning, and how to facilitate it",
    "section": "",
    "text": "Lorem ipsum.",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it"
    ]
  },
  {
    "objectID": "part2/learning-is-messy.html",
    "href": "part2/learning-is-messy.html",
    "title": "3  Learning is messy",
    "section": "",
    "text": "A key idea in Laura’s teaching philosophy is that learning is messy. But within the mess of a learner’s developing understanding will always be a nugget of something valuable. The teacher’s task is to find that nugget and build on it.\nAnother key idea: the parts of a learner’s developing understanding that aren’t worth building on should not be thought of as “wrong” or as “mistakes”.\nIf the student hasn’t learned the material yet, they cannot make a mistake. Something is only a mistake if the learner knows better. Something that might be wrong at performance time, when given as a final answer, is not wrong while someone is still learning.\nLearners must be allowed to explore ideas and take risks without their attempts being described—by themselves or by their teachers—as wrong. “Wrong” is a judgement, an evaluation, and it does not belong within the process of learning. (And as we’ll discuss in Part 3, when the time does come to assess students’ performance, it’s important to phrase our evaluations much more constructively than “this is wrong”!)",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Learning is messy</span>"
    ]
  },
  {
    "objectID": "part2/learning-takes-energy.html",
    "href": "part2/learning-takes-energy.html",
    "title": "4  Learning takes energy",
    "section": "",
    "text": "It is easier to remain in the same state than to change. And since learning is change, learning is hard work.\nFurther, learning deeply is harder work than learning superficially. This is one reason why students’ learning is often superficial. Superficial learning is characterised by just “kluging” new information on top of old information, rather than interrogating or modifying that old information in light of the new. Like hermit crabs, what changes is the shell, not what’s inside. This kluging behaviour is especially apparent when the new information conflicts with what students learned before. In such scenarios, teachers must be prepared to prompt their students to go deeper.\nTake, for example, the Grade 11 physics curriculum in Alberta, Canada. Before Grade 11 (the North American equivalent of the UK’s Year 12/Lower Sixth Form), students had been learning about motion in physical systems in terms of friction. But Grade 11 brings a paradigm shift: describing motion without considering the effect of friction. Students find leaving friction out of the equation to be hugely counter-intuitive because until this point in their education and in their real-world experiences, friction has been a major player.\nTo successfully navigate this paradigm shift, students need to update their worldview and develop a new common sense about motion in physical systems. And this takes energy. Students reveal that they’re not investing that energy, that instead they’re superficially kluging ideas onto one another, when they ask things like, “Do you want me to tell you what I think, or what you told me yesterday?”\nSo how can we help students to go deeper, to update their worldviews in light of new, conflicting information?\nWe can prompt students to make predictions, and let their predictions fail.\nPredictions reveal what students believe about how the world works. To produce a prediction, students need to recognise and commit to their beliefs. And if they predict incorrectly, they’re well positioned for us to help them see that their belief needs to be modified. People must commit to a belief (e.g., by prediction) before that belief can be updated.\nMaking and evaluating predictions is one kind of “learning by doing”. In the next section, we explore more activities that help students learn.",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Learning takes energy</span>"
    ]
  },
  {
    "objectID": "part2/learning-by-doing.html",
    "href": "part2/learning-by-doing.html",
    "title": "5  Learning happens by doing",
    "section": "",
    "text": "5.1 A resource: Tools for Teaching Science\nIn 2020, Laura co-authored a free resource called Tools for Teaching Science, together with Sean Jackson at the Perimeter Institute for Theoretical Physics (Jackson & Pankratz, 2020).\nSome example classroom activities discussed in Jackson & Pankratz (2020) are:",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Learning happens by doing</span>"
    ]
  },
  {
    "objectID": "part2/interact-w-students.html",
    "href": "part2/interact-w-students.html",
    "title": "6  Interacting with students",
    "section": "",
    "text": "6.1 Respect students\nBeing a student is a difficult, vulnerable thing. Students deserve our respect. It is crucial that we respect our students and the work they invest in learning—whether or not they’re in the room. For example, one of Laura’s rules for marking student work is never to put it on the floor. Even if that would save table space, it demeans the product of student labour, and we don’t want to send that message.\nWe must respect students’ thinking.\nWe must respect students’ time and energy.",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Interacting with students</span>"
    ]
  },
  {
    "objectID": "part2/designing-course.html",
    "href": "part2/designing-course.html",
    "title": "7  Designing a course",
    "section": "",
    "text": "7.1 Know the purpose\nIn Section 6, we advised you to know the purpose of every question you ask. The same principle holds at the level of course design: know the purpose of every piece of material you include.\nIntentionally curating what goes into your course—and then telling students why you included each concept—will help motivate students to learn the material for reasons beyond “I need this for the test”.",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Designing a course</span>"
    ]
  },
  {
    "objectID": "part3/intro-behavioural.html",
    "href": "part3/intro-behavioural.html",
    "title": "The behavioural outcomes of learning, and how to assess them",
    "section": "",
    "text": "Lorem ipsum.",
    "crumbs": [
      "The behavioural outcomes of learning, and how to assess them"
    ]
  },
  {
    "objectID": "part3/more-than-grades.html",
    "href": "part3/more-than-grades.html",
    "title": "8  More than grades",
    "section": "",
    "text": "The way that teachers typically use the word “assessment” has to do with evaluating students’ performance, giving them feedback, and usually labelling the work with a grade. Our focus on these aspects of assessment is understandable, since we’re all operating within a myopically grades-oriented system. But if the goal of education is learning (which we argue it is), then assessment can do much more for us than evaluate how students perform.\nAssessment is about more than grades. In general, assessment is done to gain new information, for the teacher or for the student. \nAssessment is about checking in on how students are doing in response to the context/situation they’re in. This idea means that assessment applies not just to examination/evaluation scenarios—we also use assessment to learn about what’s going on with the student in the moment.\nThus, assessment should be happening all the time. For example:\n\nReading student body language and learning physical tells: are students tired? bored? distracted? confused?\n\nIn particular, learn the tells of your tenth best student. This student is more likely to reflect, on average, where the class is at, and figuring out their physical tells for confusion will help you pick up on when the class is confused without making anyone feel picked on or singled out.\n\nListening to the kinds of questions that students are asking (see Section 2.2).\nIf students are off-task, figuring out whether the task is meaningless, or too easy, or too hard.\n\nAll of these checks provide useful feedback—new information—to the teacher.\nThis level of on-your-feet metacognition is really challenging, and it’s the mark of an excellent, masterful teacher. To be a good teacher, you can be a brilliant presenter with clear materials that engage your students. To take that extra step toward excellence, do all of that with the student in mind at every moment—with active metacognitive assessment of the situation.\nWith this important caveat out of the way—that assessment is about more than grades—we’ll focus for the rest of this book on how to design assessments of student performance.",
    "crumbs": [
      "The behavioural outcomes of learning, and how to assess them",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>More than grades</span>"
    ]
  },
  {
    "objectID": "part3/designing-assessment.html",
    "href": "part3/designing-assessment.html",
    "title": "9  Designing an assessment",
    "section": "",
    "text": "9.1 Three principles of assessment design\nAssessment should be fair, reliable, and valid.",
    "crumbs": [
      "The behavioural outcomes of learning, and how to assess them",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Designing an assessment</span>"
    ]
  },
  {
    "objectID": "part3/designing-question.html",
    "href": "part3/designing-question.html",
    "title": "10  Designing individual questions",
    "section": "",
    "text": "10.1 What makes an item good?\nA good item is clear, accurate, fair, and valid.",
    "crumbs": [
      "The behavioural outcomes of learning, and how to assess them",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Designing individual questions</span>"
    ]
  },
  {
    "objectID": "part3/designing-mcq.html",
    "href": "part3/designing-mcq.html",
    "title": "11  Designing multiple choice questions",
    "section": "",
    "text": "11.1 MCQs are relatively easy\nMCQs are easier to succeed on than student-generated responses are. In MCQs, all the options are right there. And even if a test-taker didn’t know a thing about Canada and chose a city purely at random, they would still correctly choose “Ottawa” one time out of four.\nSo an exam built solely of MCQs will likely have a higher rate of success than an exam built solely of student-generated responses. But this doesn’t mean that the students know more. It just means that the task is easier.",
    "crumbs": [
      "The behavioural outcomes of learning, and how to assess them",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Designing multiple choice questions</span>"
    ]
  },
  {
    "objectID": "part3/designing-sgr.html",
    "href": "part3/designing-sgr.html",
    "title": "12  Designing student-generated response tasks",
    "section": "",
    "text": "12.1 SGRs are harder than multiple choice\nStudents can succeed on multiple choice questions (MCQs), even if all they can do is recognise the correct answer among the incorrect ones. The same ability does not allow them to succeed on SGRs, where they need to produce the answer from scratch with no clues beyond the ones in the prompt. And it makes sense that this is difficult, considering that “creating” is the pinnacle of Bloom’s Taxonomy.",
    "crumbs": [
      "The behavioural outcomes of learning, and how to assess them",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Designing student-generated response tasks</span>"
    ]
  },
  {
    "objectID": "part3/formative.html",
    "href": "part3/formative.html",
    "title": "13  Ideas for formative assessment",
    "section": "",
    "text": "13.1 Peer feedback activity\nFormative feedback doesn’t need to come from the teacher. And as we discussed in Section 5, feedback from peers can sometimes be even more effective.\nIn this activity, students A and B are paired up.\n(While A is writing their response, B is also writing their own; while B is reviewing A’s work, A is reviewing B’s; and so on.)\nThe third step, where A articulates what changes they’ll make in response to B’s feedback, might feel unnecessary. But this step is crucial because often, students who don’t articulate their responses to the feedback won’t make the changes. And explicitly articulating the changes they’ll make can help students recognise that feedback indicates areas of potential growth.",
    "crumbs": [
      "The behavioural outcomes of learning, and how to assess them",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Ideas for formative assessment</span>"
    ]
  },
  {
    "objectID": "part3/summative.html",
    "href": "part3/summative.html",
    "title": "14  Considerations around summative assessment",
    "section": "",
    "text": "14.1 Dichotomous vs. partial marks\nA dichotomous marking system is one where a response can only either be marked “correct” or “incorrect”. In contrast, in a partial marking system, partial credit can be awarded for partially correct responses.\nIn Laura’s view, for student-generated responses, the use of partial marks is absolutely appropriate, because in most student-generated responses, there’s usually something at least partly good. For multiple choice questions, the decision is a bit more complex. Here are some points to keep in mind:",
    "crumbs": [
      "The behavioural outcomes of learning, and how to assess them",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Considerations around summative assessment</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Ambrose, S. A., Bridges, M. B., DiPietro, M., Lovett, M. C., &\nNorman, M. K. (2010). How learning works. Jossey-Bass.\n\n\nAnderson, L. W., & Krathwohl, D. R. (2001). A taxonomy for\nlearning, teaching, and assessing: A revision of\nBloom’s taxonomy of educational objectives. Longman.\n\n\nBiggs, J. B., & Collis, K. F. (1982). Evaluating the quality of\nlearning: The SOLO taxonomy (Structure of the\nObserved Learning Outcome). Academic Press, Inc.\n\n\nBlackwell, L. S., Trzesniewski, K. H., & Dweck, C. S. (2007).\nImplicit Theories of Intelligence Predict Achievement\nAcross an Adolescent Transition: A\nLongitudinal Study and an Intervention. Child\nDevelopment, 78(1), 246–263. https://doi.org/10.1111/j.1467-8624.2007.00995.x\n\n\nBloom, B., Engelhart, M. D., Furst, E. J., Hill, W. H., & Krathwohl,\nD. R. (1956). Taxonomy of educational objectives: The\nclassification of educational goals. Longman.\n\n\nBrown, P. C., Roediger, H. L., & McDaniel, M. A. (2014). Make it\nstick: The science of successful learning. The Belknap\nPress of Harvard University Press.\n\n\nButler, R., & Nisan, M. (1986). Effects of no feedback, task-related\ncomments, and grades on intrinsic motivation and performance.\nJournal of Educational Psychology, 78(3), 210.\n\n\nClark, D., & Talbert, R. (2023). Grading for growth: A guide to\nalternative grading practices that promote authentic learning and\nstudent engagement in higher education. Routledge.\n\n\nDeslauriers, L., McCarty, L. S., Miller, K., Callaghan, K., &\nKestin, G. (2019). Measuring actual learning versus feeling of learning\nin response to being actively engaged in the classroom. Proceedings\nof the National Academy of Sciences, 116(39), 19251–19257.\nhttps://doi.org/10.1073/pnas.1821936116\n\n\nDweck, C. S. (2006). Mindset: The new psychology of success.\nRandom house.\n\n\nİnan-Kaya, G., & Rubie-Davies, C. M. (2022). Teacher classroom\ninteractions and behaviours: Indications of bias.\nLearning and Instruction, 78, 101516. https://doi.org/10.1016/j.learninstruc.2021.101516\n\n\nJackson, S., & Pankratz, L. (2020). Tools for Teaching\nScience. Perimeter Institute for Theoretical Physics.\n\n\nKahnemann, D. (2011). Thinking fast and slow. Farrar,\nStraus and Giroux.\n\n\nMangen, A., Anda, L. G., Oxborough, G. H., & Brønnick, K. (2015).\nHandwriting versus keyboard writing: Effect on word recall.\nJournal of Writing Research, 7(2), 227–247. https://doi.org/10.17239/jowr-2015.07.02.1\n\n\nSambell, K., McDowell, L., & Montgomery, C. (2012). Assessment\nfor learning in higher education. Routledge.\n\n\nSmoker, T. J., Murphy, C. E., & Rockwell, A. K. (2009). Comparing\nMemory for Handwriting versus\nTyping. Proceedings of the Human Factors and Ergonomics\nSociety Annual Meeting, 53(22), 1744–1747. https://doi.org/10.1177/154193120905302218\n\n\nTullis, J. G., & Goldstone, R. L. (2020). Why does peer instruction\nbenefit student learning? Cognitive Research: Principles and\nImplications, 5(1), 5–15. https://doi.org/10.1186/s41235-020-00218-5",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "part1/learning-taxonomies.html#blooms-taxonomy",
    "href": "part1/learning-taxonomies.html#blooms-taxonomy",
    "title": "2  Taxonomies of learning",
    "section": "",
    "text": "Remember\nUnderstand\nApply\nAnalyze\nEvaluate\nCreate",
    "crumbs": [
      "What is learning?",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Taxonomies of learning</span>"
    ]
  },
  {
    "objectID": "part2/learning-by-doing.html#tools-for-teaching-science",
    "href": "part2/learning-by-doing.html#tools-for-teaching-science",
    "title": "5  Learning happens by doing",
    "section": "",
    "text": "Jigsaws (a structure for lively group work)\nPredict, Explain, Observe, Explain (PEOE)\nMany formats of student voting",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Learning happens by doing</span>"
    ]
  },
  {
    "objectID": "part2/learning-by-doing.html#more-ideas",
    "href": "part2/learning-by-doing.html#more-ideas",
    "title": "5  Learning happens by doing",
    "section": "5.2 More ideas",
    "text": "5.2 More ideas\n\nLaura has observed that attention and cognition is improved for about ten minutes following physical activity. So to engage this boost, ask students to move—for example, by voting on a prompt by moving to the four corners of the room.\nAsking students to improve an existing response is a useful activity for prompting meta-cognition. If the example response was produced by the teacher, though, then the activity is less effective than if the example response comes from a student. Starting with the teacher’s response may make students feel like the response is already above their level and that they’ll never be that good. (Food for thought: what about an example response produced by a large language model?)\nVoting:\n\nVoting activities are good for quicker thinkers and less kind to slower thinkers. To give slower thinkers time to catch up, consider modifying the format to give some pre-vote thinking time or using the format of Think–Pair–Share.\nAllow students to change their vote, to model the process of learning.\n\nPeer interaction:\n\nStudents learn better from one another than they learn from us (Tullis & Goldstone, 2020). But peer instruction especially improves the learning of the top students, in Laura’s experience, because the top students have the chance to explain a concept to somebody else—and explaining things deepens understanding. To make sure that struggling students still benefit from peer instruction, be transparent about how it will benefit them: for example, that they’ll be exposed to the concept explained in a different way.\nWhen students are asking one another questions, listen in:\n\nWhat is the cognitive level of the questions they’re asking? (See SOLO taxonomy.)\nWhat type of challenge is the student trying to address?\nIs their question in line with the kind of thinking you were aiming to provoke?\n\nDuring peer interaction, can prompt them with certain questions to model active listening, for example:\n\nDo you agree with X response?\nHow would you improve that response?\nWhat else could we consider?\nWas X response better than Y response?\nWhat did Y response add to X?\nWhat else could we add?\n… and so on.\n\n\nA flipped classroom (where students prepare the material on their own time and the synchronous gatherings are for advancing students’ thinking) requires that students buy in and show their own initiative.",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Learning happens by doing</span>"
    ]
  },
  {
    "objectID": "part2/learning-by-doing.html#if-students-dont-want-to-engage",
    "href": "part2/learning-by-doing.html#if-students-dont-want-to-engage",
    "title": "5  Learning happens by doing",
    "section": "5.3 If students don’t want to engage",
    "text": "5.3 If students don’t want to engage\nThere are two big reasons why students may not want to engage in active learning: it’s hard, and it’s risky.\nLearning is hard to begin with, and active learning is harder than passive learning, as we discussed in Section 4. But what’s more, when students see material presented clearly, they may have the illusion of understanding, the illusion of mastery—whereas active learning may feel like hard, confusing work that’s not paying off. However, students should learn not to put too much stock in that feeling. Research has shown that, even if people feel like they’re learning less, they learn better and retain their learning longer from active learning and active engagement (Brown et al., 2014; Deslauriers et al., 2019). Students may be encouraged by seeing these findings.\nActive learning is risky because it means that students must potentially demonstrate that they have misunderstood something. And although misunderstandings are an intrinsic part of growth and learning and must not be thought of as “wrong” (see Section 3), in the traditional points-and-percentages approach to grading, misunderstandings are punished. So students are naturally disincentivised to reveal they have anything less than a perfect understanding.\nIdeally, students would be able to learn and we would be able to teach without the roadblocks that traditional punitive assessment places in our collective path. But most of us work within systems that prevent us from adopting more generous and compassionate assessment methods (e.g., Clark & Talbert, 2023; Sambell et al., 2012) wholecloth. Nevertheless, there are still ways in which teachers can help embolden students to take risks. In particular, students will be more willing to go out on a limb and try something hard if they trust the teacher: if they trust that they really can succeed at whatever task the teacher has set for them, and trust that whatever they produce, the teacher will receive it with respect.\nNext, in Section 6, we’ll discuss in more detail the relationship between student and teacher, including the building of this trust.\n\n\n\n\nBrown, P. C., Roediger, H. L., & McDaniel, M. A. (2014). Make it stick: The science of successful learning. The Belknap Press of Harvard University Press.\n\n\nClark, D., & Talbert, R. (2023). Grading for growth: A guide to alternative grading practices that promote authentic learning and student engagement in higher education. Routledge.\n\n\nDeslauriers, L., McCarty, L. S., Miller, K., Callaghan, K., & Kestin, G. (2019). Measuring actual learning versus feeling of learning in response to being actively engaged in the classroom. Proceedings of the National Academy of Sciences, 116(39), 19251–19257. https://doi.org/10.1073/pnas.1821936116\n\n\nJackson, S., & Pankratz, L. (2020). Tools for Teaching Science. Perimeter Institute for Theoretical Physics.\n\n\nMangen, A., Anda, L. G., Oxborough, G. H., & Brønnick, K. (2015). Handwriting versus keyboard writing: Effect on word recall. Journal of Writing Research, 7(2), 227–247. https://doi.org/10.17239/jowr-2015.07.02.1\n\n\nSambell, K., McDowell, L., & Montgomery, C. (2012). Assessment for learning in higher education. Routledge.\n\n\nSmoker, T. J., Murphy, C. E., & Rockwell, A. K. (2009). Comparing Memory for Handwriting versus Typing. Proceedings of the Human Factors and Ergonomics Society Annual Meeting, 53(22), 1744–1747. https://doi.org/10.1177/154193120905302218\n\n\nTullis, J. G., & Goldstone, R. L. (2020). Why does peer instruction benefit student learning? Cognitive Research: Principles and Implications, 5(1), 5–15. https://doi.org/10.1186/s41235-020-00218-5",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Learning happens by doing</span>"
    ]
  },
  {
    "objectID": "part2/interact-w-students.html#respect-the-student",
    "href": "part2/interact-w-students.html#respect-the-student",
    "title": "6  Interacting with students",
    "section": "",
    "text": "When they ask you a question or respond to one of yours, really listen and think about, e.g., what cognitive level (see SOLO taxonomy) they are targeting. More on this below. If you can, use their response to move to the next idea.\nFind the nugget in what they’ve produced that is valid and valuable and build on it in the target direction.\n\nFor example, “X and Y are good points! How would we move between them?”\nSuch a response gives a constructive direction to the thinking, rather than demeaning the response (or worse, the person).\n\nStudents are not attempting to be obtuse. If their response doesn’t seem reasonable, then consider that the prompt might have been unclear, rather than that their thinking is lacking.\nFocus on what the student has done well, what progress they have made.\n\n\n\nMake assessments meaningful and relevant.\nUse students’ time intentionally and wisely (more on this in the next chapter).",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Interacting with students</span>"
    ]
  },
  {
    "objectID": "part2/interact-w-students.html#build-students-trust",
    "href": "part2/interact-w-students.html#build-students-trust",
    "title": "6  Interacting with students",
    "section": "6.2 Build student’s trust",
    "text": "6.2 Build student’s trust\nLaura thinks of trust in terms of “equity” in the financial sense. If equity is good, that is, if the trust in the student–teacher relationship is good, then the teacher can challenge and push the students and the students will go along.\nTo build good equity, to nurture that trust, students must believe two things.\n\nStudents must believe that you’re asking them to do something they can actually succeed at, not something impossible.\nStudents must know that you’ll respect whatever response they give you (see above).\n\nLaura likes the financial metaphor because teacher equity can also be spent. For example, when we write a hard question or set a difficult task, it costs some equity, because the students need to take a risk. Teachers must be intentional about when and how much this equity is spent.\nIn the marks economy, students only want to be successful—they don’t want to learn. Good teacher equity can help motivate students to learn despite this, to take risks anyway, because the students trust that they will be successful. But the reverse holds, too: if students aren’t trying, then that indicates that they don’t trust the teacher—that there’s insufficient equity to do what the teacher is aiming to do.",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Interacting with students</span>"
    ]
  },
  {
    "objectID": "part2/interact-w-students.html#welcome-questions",
    "href": "part2/interact-w-students.html#welcome-questions",
    "title": "6  Interacting with students",
    "section": "6.3 Welcome questions",
    "text": "6.3 Welcome questions\nReally listen to the questions.",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Interacting with students</span>"
    ]
  },
  {
    "objectID": "part2/interact-w-students.html#be-intentional-about-the-questions-you-ask",
    "href": "part2/interact-w-students.html#be-intentional-about-the-questions-you-ask",
    "title": "6  Interacting with students",
    "section": "6.4 Be intentional about the questions you ask",
    "text": "6.4 Be intentional about the questions you ask",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Interacting with students</span>"
    ]
  },
  {
    "objectID": "part2/interact-w-students.html#build-trust",
    "href": "part2/interact-w-students.html#build-trust",
    "title": "6  Interacting with students",
    "section": "6.2 Build trust",
    "text": "6.2 Build trust\nLaura thinks of trust in terms of “equity” in the financial sense. If equity is good—that is, if students’ trust in their teacher is good—then the teacher can challenge and push the students and the students will go along.\nTo build good equity, to nurture that trust, students must believe two things.\n\nStudents must believe that you’re asking them to do something they can actually succeed at, not something impossible.\nStudents must know that you’ll respect whatever response they give you (see above).\n\nLaura likes the financial metaphor because teacher equity can also be spent. For example, when we write a hard question or set a difficult task, it costs some equity, because the students need to take a risk. Teachers must be intentional about when and how much this equity is spent.\nIn the marks economy, students only want to be successful—they don’t want to learn. Good teacher equity can help motivate students to learn despite this, to take risks anyway, because the students trust that they will be successful. But the reverse holds, too: if students aren’t trying, then that indicates that they don’t trust the teacher—that there’s insufficient equity to do what the teacher is aiming to do.",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Interacting with students</span>"
    ]
  },
  {
    "objectID": "part2/interact-w-students.html#respect-students",
    "href": "part2/interact-w-students.html#respect-students",
    "title": "6  Interacting with students",
    "section": "",
    "text": "Students are not attempting to be obtuse. If their response doesn’t seem reasonable, then consider that the prompt might have been unclear, rather than that the students’ thinking is lacking.\nFind the valuable idea in what they produce and, if you can, use their response to move to the next idea or pivot in the target direction. For example:\n\n“X and Y are good points! How would we move between them?”\n“Your example is a good illustration of X. How else can we see X here?”\n\nFocus on what the student has done well, what progress they have made.\n\n\n\nMake assessments fair, valid, reliable, meaningful, and relevant (more on this in Section 9).\nUse students’ cognitive capacity intentionally and wisely (more on this in Section 7).",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Interacting with students</span>"
    ]
  },
  {
    "objectID": "part2/interact-w-students.html#welcome-questions-from-students",
    "href": "part2/interact-w-students.html#welcome-questions-from-students",
    "title": "6  Interacting with students",
    "section": "6.3 Welcome questions from students",
    "text": "6.3 Welcome questions from students\nA student asking you a question is a victory for you and for them! A question is evidence that the student is following along, and evidence that they feel safe enough to show that they don’t understand something. So questions from students are something to be celebrated.\nMoreover, we can learn a lot about where a student’s at by analysing the kinds of questions they ask. What cognitive level (see Section 2.2) is the student trying to use? What is their attitude toward risk? What’s their resilience like?\nEven a hugely broad question like “What should I do? I don’t know where to start” tells you, for example, that the student wants a memorised, known, algorithmic method to apply that will always work. If memorising algorithms is the goal of the course, then giving exactly that kind of guidance is appropriate. But if that’s not the goal of the course, then this becomes an opportunity to guide the student toward the answer.\nIn general, when a student asks for help, try not to tell. Try instead to elicit, for example with Socratic questioning.",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Interacting with students</span>"
    ]
  },
  {
    "objectID": "part2/interact-w-students.html#socratic-questioning",
    "href": "part2/interact-w-students.html#socratic-questioning",
    "title": "6  Interacting with students",
    "section": "6.4 Socratic questioning",
    "text": "6.4 Socratic questioning\nStudents find Socratic questioning very annoying, but it is very effective. Socratic questions aren’t just any questions directed toward the student: “Why did you do that?” is not a Socratic question. Rather, Socratic questions aim to prompt metacognition.\nA good starting point is “Can you walk me through your thinking so far?” By going step by step through their thinking, students will often identify their own missteps. It’s great when students do this because the teacher doesn’t need to point out where the misunderstanding was–the student has done the metacognitive troubleshooting themselves and has gained an effective strategy for their toolkit.\nA good strategy for Socratic questioning is to start at the lowest cognitive level in the SOLO taxonomy, then if the student meets you at that level, ramp up the complexity. Start with uni-structural questions to check whether students understand individual ideas (e.g., “Can you tell me about X?”). If they do, then progress to multi-structural and relational questions, guiding the student to connect their ideas in the target direction (e.g., “Why do you think X is happening?”, “What’s one way X is related to Y?”)",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Interacting with students</span>"
    ]
  },
  {
    "objectID": "part2/interact-w-students.html#ask-questions-intentionally",
    "href": "part2/interact-w-students.html#ask-questions-intentionally",
    "title": "6  Interacting with students",
    "section": "6.6 Ask questions intentionally",
    "text": "6.6 Ask questions intentionally\nEvery question you ask (and indeed, every task and activity you set) should have a clear intention, purpose, or goal. You should know what your question is trying to accomplish, so that you can judge whether the aim has been met.\nFor example, you might ask questions for the following reasons:\n\nAs a management strategy: to wake students up, to get them on task, to redirect a behaviour.\nTo gauge the level understanding in the class.\n\nBest to target these questions not toward the top students, but toward students at about the 80th percentile. Their understanding is more likely representative of the class overall.\nDon’t ask “Are there any questions?”, but “At this point, what questions do you have?”\nDon’t ask “Do you understand?”, but “What’s next?” or “In this situation, what would you do?” (People don’t like admitting to not understanding.)\n\nVery broadly: to gain information that the teacher (and potentially the students) didn’t previously have.\n\nBy responding to student questions in a particular way, you can make students an authority in the classroom too. Specifically, you can allow students to thoroughly answer questions and then just move on to the next idea, not restating what the student already said. Doing this signals to other students that knowledge can come not just from the teacher but also from their peers. It changes the role of student and teacher in the classroom, and it unnerves students who expect to get all their information from the teacher. However, learning from their peers is an important skill for students to develop, and this is one way to encourage it.",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Interacting with students</span>"
    ]
  },
  {
    "objectID": "part2/interact-w-students.html#attend-to-the-cognitive-level-of-your-questions",
    "href": "part2/interact-w-students.html#attend-to-the-cognitive-level-of-your-questions",
    "title": "6  Interacting with students",
    "section": "6.5 Attend to the cognitive level of your questions",
    "text": "6.5 Attend to the cognitive level of your questions\nWhen asking any kind of question, teachers must be aware of what cognitive level they are asking about, and what level of response they’re getting back. (This ties back to the importance of listening, mentioned above.)\nIf you ask a uni-structural question where a student just has to recall an idea, and they give you a uni-structural answer back, you can be happy. But if you ask a relational question and they give you a uni-structural answer, then don’t fall prey to confirmation bias (more below)—double down and clarify what kind of response you wanted from them.\nAim to ask students questions that are in their ballpark, that are roughly at their level of ability. There’s no point asking someone who’s already struggling a very challenging question. But be aware of internalised biases in terms of whom you assume to be operating at particular cognitive levels (İnan-Kaya & Rubie-Davies, 2022).\nIn fact, it’s very useful to tell students about the SOLO taxonomy, tell them that there are different levels of cognition that we’re applying. If students are familiar with these cognitive levels, a very handy metacognitive level of conversation opens up. For example, you can say, “I’m about to ask you a relational question”, and the student might say, “OK, but I might only give you a uni-structural response”. This is great, because the student has understood what’s being asked and what cognitive level they’re at! Even better when the students can articulate, say, “I have a multi-structural level of understanding but I want a relational one”. We want students to be able to manage their own learning, and this is one useful way.\nAdditionally, telling people what level of cognition you want from them will change the way they approach a question (Kahnemann, 2011). Even without using the SOLO terminology, you might say something like, “I’m going to ask a question that requires you to think, so I’m not going to take the first answer.” And telling students up front that you want them to think will make them more likely to engage more complex levels of cognition.",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Interacting with students</span>"
    ]
  },
  {
    "objectID": "part2/interact-w-students.html#think-of-the-cognitive-level-of-your-questions",
    "href": "part2/interact-w-students.html#think-of-the-cognitive-level-of-your-questions",
    "title": "6  Interacting with students",
    "section": "6.5 Think of the cognitive level of your questions",
    "text": "6.5 Think of the cognitive level of your questions\nEven beyond of the Socratic method, when asking any kind of question, teachers must be aware of what cognitive level they are targeting and whether they are getting back that same level of response. (This ties back to the importance of listening, mentioned above.)\nIf you ask a uni-structural question where a student just has to recall an idea, and they give you a uni-structural answer back, you can be happy. But if you ask a relational question and they give you a uni-structural answer, then don’t fall prey to confirmation bias (more below)—double down and clarify what kind of response you wanted from them.\nAim to ask students questions that are in their ballpark, that are roughly at their level of ability. There’s no point asking someone who’s already struggling a very challenging question. But be aware of internalised biases in terms of whom you assume to be operating at what cognitive level (İnan-Kaya & Rubie-Davies, 2022).\nIn fact, it’s very useful to tell students about the SOLO taxonomy, tell them that there are different levels of cognition that we’re applying. If students are familiar with these cognitive levels, a very handy metacognitive level of conversation opens up. For example, you can say, “I’m about to ask you a relational question”, and the student might say, “OK, but I might only give you a uni-structural response”. This is great, because the student has understood what’s being asked and what cognitive level they’re at! Even better when the students can articulate, say, “I have a multi-structural level of understanding but I want a relational one”. We want students to be able to manage their own learning, and this is one useful way.\nAdditionally, telling people what level of cognition you want from them will change the way they approach a question (Kahnemann, 2011). Even without using the SOLO terminology, you might say something like, “I’m going to ask a question that requires you to think, so I’m not going to take the first answer.” And telling students up front that you want them to think will make them more likely to engage more complex levels of cognition.",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Interacting with students</span>"
    ]
  },
  {
    "objectID": "part2/interact-w-students.html#confirmation-bias",
    "href": "part2/interact-w-students.html#confirmation-bias",
    "title": "6  Interacting with students",
    "section": "6.7 Confirmation bias",
    "text": "6.7 Confirmation bias\nIf a student answers our question in an appropriate way, then we tend to assume that they’re following the logic we wanted them to follow to get to that answer. If we don’t question this assumption, we fall prey to confirmation bias.\nConfirmation bias can prevent opportunities for learning. For example, when a student gives us the response we want, we tend to stop pushing. In this way, a good answer from a student can end an entire discussion. And if the class as a whole isn’t there yet, then the star student might end the learning moment before the rest of the class is ready to move on. So we have to be aware of the the range of understanding in the class as a whole, and we have to make sure that our confirmation bias doesn’t make us move on faster than we should.\n\n\n\n\nİnan-Kaya, G., & Rubie-Davies, C. M. (2022). Teacher classroom interactions and behaviours: Indications of bias. Learning and Instruction, 78, 101516. https://doi.org/10.1016/j.learninstruc.2021.101516\n\n\nKahnemann, D. (2011). Thinking fast and slow. Farrar, Straus and Giroux.",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Interacting with students</span>"
    ]
  },
  {
    "objectID": "part2/designing-course.html#transparency",
    "href": "part2/designing-course.html#transparency",
    "title": "7  Designing a course",
    "section": "7.2 Transparency",
    "text": "7.2 Transparency\n\n\n\n\nAmbrose, S. A., Bridges, M. B., DiPietro, M., Lovett, M. C., & Norman, M. K. (2010). How learning works. Jossey-Bass.\n\n\nBiggs, J. B., & Collis, K. F. (1982). Evaluating the quality of learning: The SOLO taxonomy (Structure of the Observed Learning Outcome). Academic Press, Inc.\n\n\nBrown, P. C., Roediger, H. L., & McDaniel, M. A. (2014). Make it stick: The science of successful learning. The Belknap Press of Harvard University Press.",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Designing a course</span>"
    ]
  },
  {
    "objectID": "part2/designing-course.html#breaking-down-silos",
    "href": "part2/designing-course.html#breaking-down-silos",
    "title": "7  Designing a course",
    "section": "",
    "text": "7.1.1 Interleaving\nInterleaving refers to switching between topics within some span of time, rather than focusing only on one topic and then moving on to the next.\nUnlike a silo structure such as\n\nDay 1: Topic A\nDay 2: Topic A\nDay 3: Topic B\nDay 4: Topic B,\n\nan interleaved structure might be\n\nDay 1: Topic A\nDay 2: Topic B\nDay 3: Topic A\nDay 4: Topic B.\n\nReturning to Topic A after Topic B has been opened invites learners to draw connections between A and B that they may otherwise have missed.\nInterleaving has been shown to strengthen learning and aid retention (Brown et al., 2014). It’s cognitively harder work for both student and teacher, but it leads to better learning long-term.\n\n\n7.1.2 Velcro teaching\nVelcro teaching is Laura’s term for the process of explicitly drawing connections between material in different sections of a course. You might set the hook forward (“next week, we’ll hear more about X”), and if you do, you’ll have to deliver (“like I promised last week, we’re going to talk more about X”). We’ve been setting hooks in this text, too, every time we say “We’ll talk more about X in Chapter Y”. For example, in Chapter 2 when we set the hook forward about all the places the SOLO taxonomy will reappear throughout this text.\nVelcro teaching requires that you know your material at least a week ahead of time, and that you also have a solid understanding of how the topics in your course fit together. This level of understanding can be difficult to attain the first couple times through a course. Give yourself grace! Sometimes just making it through the week is enough.\nBut the goal of velcro teaching is to help students keep the boxes of each topic open and to prepare them for making the connections that you think are important.",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Designing a course</span>"
    ]
  },
  {
    "objectID": "part2/designing-course.html#transparency-about-structure",
    "href": "part2/designing-course.html#transparency-about-structure",
    "title": "7  Designing a course",
    "section": "7.3 Transparency about structure",
    "text": "7.3 Transparency about structure\nHowever you choose to structure your course, tell students as early as possible what they should expect. Telling people what’s coming before they experience it is called “pre-organising”. Novice learners and the average person function well in a pre-organised environment. So being transparent about what you have planned and how it should all fit together in advance can help get students on board.\nThe alternative to pre-organising is post-organising: giving people an experience and then helping them put a structure over it in retrospect. Post-organising is better-suited for more masterful learners, people who are more comfortable juggling several live ideas at once.\nTransparency about structure also applies at the level of individual lessons. At the beginning of a lesson, give students an outline of what they can expect from that session. And importantly, check off topics from the outline as they’re accomplished. The goal is for everybody to be able to tell where they are in the day’s schedule, even if (when) their attention falters. People should always be able to find a way back in.\n\nIn the last several sections, we have been focusing on the cognitive process of learning and how we as educators can facilitate it.\nIn Section 8, we pivot to the behavioural outcomes of learning and how we can assess them.\n\n\n\n\nAmbrose, S. A., Bridges, M. B., DiPietro, M., Lovett, M. C., & Norman, M. K. (2010). How learning works. Jossey-Bass.\n\n\nBiggs, J. B., & Collis, K. F. (1982). Evaluating the quality of learning: The SOLO taxonomy (Structure of the Observed Learning Outcome). Academic Press, Inc.\n\n\nBrown, P. C., Roediger, H. L., & McDaniel, M. A. (2014). Make it stick: The science of successful learning. The Belknap Press of Harvard University Press.",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Designing a course</span>"
    ]
  },
  {
    "objectID": "part2/designing-course.html#be-selective-about-content",
    "href": "part2/designing-course.html#be-selective-about-content",
    "title": "7  Designing a course",
    "section": "7.2 Be selective about content",
    "text": "7.2 Be selective about content\nAs we mentioned back in Section 4, lorem ipsum.",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Designing a course</span>"
    ]
  },
  {
    "objectID": "part2/designing-course.html#break-down-silos",
    "href": "part2/designing-course.html#break-down-silos",
    "title": "7  Designing a course",
    "section": "7.2 Break down silos",
    "text": "7.2 Break down silos\nCourses tend to be organised into units. Each unit gets its own box, and the material in that box is tidily compartmentalised away from the material in other boxes. When each unit is finished, that box is closed up, and students and teacher move on to the next one. Laura calls this practice “siloing”.\nSiloing is a common way to organise a course, but it’s not a good way. When the material in each unit is handled separately, learners cannot forge robust connections between ideas and build out their mental network of conceptual links—one of the most important tasks in deepening their understanding (Ambrose et al., 2010; Biggs & Collis, 1982). Keeping all the boxes open, even after the units are past, is a harder cognitive task, but it’s a better education.\nTo avoid siloing, two methods that teachers can use are interleaving material and what Laura calls Velcro teaching.\n\n7.2.1 Interleaving\nRather than focusing only on one topic, discussing it exhaustively, and then moving on to the next one without referring back to earlier ideas, by interleaving, we switch back and forth between topics.\nSo, unlike a silo structure such as\n\nDay 1: Topic A\nDay 2: Topic A\nDay 3: Topic B\nDay 4: Topic B,\n\nan interleaved structure might be\n\nDay 1: Topic A\nDay 2: Topic B\nDay 3: Topic A\nDay 4: Topic B.\n\nReturning to Topic A after Topic B has been opened invites learners to draw connections between A and B that they may otherwise have missed.\nInterleaving has been shown to strengthen learning and aid retention (Brown et al., 2014). It’s cognitively harder work for both student and teacher, but it improves learning long-term.\n\n\n7.2.2 Velcro teaching\nVelcro teaching is Laura’s term for the process of explicitly drawing connections between material in different sections of a course. You can set the hook forward (“Next week, we’ll learn about Y and see how it connects to X”) and then deliver on that hook (“Remember X from last week? Here’s Y, and now we’re going to learn how X and Y connect.”). We’ve been setting hooks in this text, too, for example back in Section 2, when we mentioned all the places the SOLO taxonomy will reappear.\nHooks aren’t always helpful, though. Sometimes setting a hook forward can be dissatisfying, especially when students have no frame of reference for what the topic will be: foreshadowing that “we’ll hear more about [confusing unfamiliar jargon] later” can frustrate students in a way similar to “you’ll understand when you’re older”. So use your judgement about when providing information up front would offer helpful structure (see below) and when it might just confuse things further.\nThe practice of drawing connections between older and newer ideas—that is, the practice of Velcro teaching—requires you to\n\nknow your material at least a week ahead of time, so that you can set hooks forward, and\nunderstand the range of ways in which the topics in your course can be linked together.\n\nThe first couple times through a course, this level of understanding can be hard to reach. So give yourself grace—early on, making it through the week is enough. Even if it’s not every session, any hook that is set and delivered on can help students forge important connections between ideas. And any time that you can ask students to bring earlier ideas into a new context will challenge them but, ultimately, strengthen their learning.",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Designing a course</span>"
    ]
  },
  {
    "objectID": "part2/learning-by-doing.html#a-resource-tools-for-teaching-science",
    "href": "part2/learning-by-doing.html#a-resource-tools-for-teaching-science",
    "title": "5  Learning happens by doing",
    "section": "",
    "text": "Student voting\nPeer instruction\nSelf-evaluation\nPredict, Explain, Observe, Explain (PEOE) activity\nKnow–Wonder–Learned charts\nJigsaw activity (a structure for lively group work)",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Learning happens by doing</span>"
    ]
  },
  {
    "objectID": "part2/learning-by-doing.html#some-more-ideas",
    "href": "part2/learning-by-doing.html#some-more-ideas",
    "title": "5  Learning happens by doing",
    "section": "5.2 Some more ideas",
    "text": "5.2 Some more ideas\n\nLaura has observed that attention and cognition is improved for about ten minutes following physical activity. So to engage this boost, ask students to move—for example, voting on a prompt by moving to the four corners of the room, or changing seats to work with a new partner.\nAsking students to improve an existing response is a useful activity for prompting metacognition. Improving an existing response is especially effective if the original response is from a student, rather than from a teacher (since correcting a teacher’s work could feel awkward, or if the work is already very good, students might feel that they can’t live up to that standard).\nA flipped classroom (where students prepare the material on their own time and the synchronous gatherings aim to advance students’ thinking) only works if students buy in and show initiative and if the teacher is good at presenting material clearly without an audience.\nIn a voting activity:\n\nTo model the process of learning, allow students to change their vote.\nIn-class live voting is fine for quicker thinkers, but it’s less kind to slower thinkers. To give slower thinkers time to catch up, consider including some thinking time before the vote, or modifying a Think–Pair–Share activity into Think–Pair–Vote.\n\n\n\n\nIn peer interaction (e.g., shoulder buddies or small informal groups):\n\nStudents learn better from one another than they learn from us (Tullis & Goldstone, 2020). But peer instruction especially improves the learning of the top students, in Laura’s experience, because the top students have the chance to explain a concept to somebody else—and explaining things deepens understanding. To make sure that struggling students still benefit from peer instruction, be transparent about how it will serve them: for example, that they’ll see the concept explained in a different way.\nWhen students are asking one another questions, listen in:\n\nWhat is the cognitive level of the questions they’re asking? (See Section 2.2.)\nWhat type of challenge is the student trying to address?\nIs their question in line with the kind of thinking you were aiming to provoke?\n\nTo interact with students while they’re interacting with their peers (e.g., in shoulder buddies or small informal groups), Laura mostly uses prompts like a raised eyebrow or a tipped head to indicate that something more is required or that something is a bit off. To intervene more strongly, she might say something like, “Walk me through your thinking about [topic]” (see Section 6.4).\n\n\nDuring peer interaction, teachers can ask students questions to model active listening, for example:\n\nDo you agree with X response?\nHow would you improve X response?\nWhat else could we consider?\nWas X response better than Y response?\nWhat did Y response add to X?\nWhat else could we add?\n… and so on.",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Learning happens by doing</span>"
    ]
  },
  {
    "objectID": "part2/learning-by-doing.html#if-students-dont-want-to-engage-build-trust",
    "href": "part2/learning-by-doing.html#if-students-dont-want-to-engage-build-trust",
    "title": "5  Learning happens by doing",
    "section": "5.3 If students don’t want to engage, build trust",
    "text": "5.3 If students don’t want to engage, build trust\nStudents often prefer a passive learning environment. It demands from them little effort and moreover, when the material is presented clearly, students may have the illusion of understanding, the illusion of mastery. And on the other hand, active learning takes hard work that can feel to many students like it’s not paying off. However, they shouldn’t put too much stock in this feeling. Research has shown that, even if students feel like they’re learning less, active engagement actually improves learning and long-term retention (Brown et al., 2014; Deslauriers et al., 2019). Encourage students by sharing these findings.\nStudents may also resist active learning because it often means taking a risk (because learning is messy!). And if students aren’t willing to take risks, this could reflect that they don’t have enough trust in the teacher: trust that they can accomplish whatever task the teacher has set for them, and trust that the teacher will receive whatever they produce with respect. In the next section, we’ll discuss in more detail the relationship between student and teacher, including building this trust.\n\n\n\n\nBrown, P. C., Roediger, H. L., & McDaniel, M. A. (2014). Make it stick: The science of successful learning. The Belknap Press of Harvard University Press.\n\n\nDeslauriers, L., McCarty, L. S., Miller, K., Callaghan, K., & Kestin, G. (2019). Measuring actual learning versus feeling of learning in response to being actively engaged in the classroom. Proceedings of the National Academy of Sciences, 116(39), 19251–19257. https://doi.org/10.1073/pnas.1821936116\n\n\nMangen, A., Anda, L. G., Oxborough, G. H., & Brønnick, K. (2015). Handwriting versus keyboard writing: Effect on word recall. Journal of Writing Research, 7(2), 227–247. https://doi.org/10.17239/jowr-2015.07.02.1\n\n\nSmoker, T. J., Murphy, C. E., & Rockwell, A. K. (2009). Comparing Memory for Handwriting versus Typing. Proceedings of the Human Factors and Ergonomics Society Annual Meeting, 53(22), 1744–1747. https://doi.org/10.1177/154193120905302218\n\n\nTullis, J. G., & Goldstone, R. L. (2020). Why does peer instruction benefit student learning? Cognitive Research: Principles and Implications, 5(1), 5–15. https://doi.org/10.1186/s41235-020-00218-5",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Learning happens by doing</span>"
    ]
  },
  {
    "objectID": "part1/learning-taxonomies.html#sec-solo",
    "href": "part1/learning-taxonomies.html#sec-solo",
    "title": "2  Taxonomies of learning",
    "section": "2.2 The SOLO taxonomy",
    "text": "2.2 The SOLO taxonomy\nSOLO stands for “Structure of the Observed Learning Outcome”. This taxonomy was developed by Biggs & Collis (1982), and it specifies five levels of increasingly complex thinking and understanding:\n\nPre-structural: The learner hasn’t started learning.\nUni-structural: The learner can describe an idea or do a single-step procedure.\nMulti-structural: The learner has a grasp on several distinct ideas but doesn’t know how they fit together.\nRelational: The learner can link, classify, sequence, compare, and contrast their ideas.\nExtended abstract: The learner can generalise from their ideas and make predictions about new contexts.\n\nSOLO’s major strength is that it describes relationships among ideas, rather than Bloom’s approach of classifying actions. And relationships among ideas are crucial for our work as educators: we must always be aware of the way students are linking (or not yet linking) their ideas.\nWe’ll come back to these levels of cognition again and again throughout this book, because Laura uses the SOLO taxonomy in many different areas of her teaching. For example:\n\nto interpret the cognitive level of questions from students,\nto decide on the appropriate level of answer to provide,\nto evaluate the level of a student’s answer compared to the level expected by the question,\nto choose the scope of a task, and\nto determine expectations of an assessment or a course.\n\nThe web site Inspiring Inquiry is a very good resource for exploring the SOLO taxonomy in greater depth.\n\n\n\n\n\n\n\n\n\nAnderson, L. W., & Krathwohl, D. R. (2001). A taxonomy for learning, teaching, and assessing: A revision of Bloom’s taxonomy of educational objectives. Longman.\n\n\nBiggs, J. B., & Collis, K. F. (1982). Evaluating the quality of learning: The SOLO taxonomy (Structure of the Observed Learning Outcome). Academic Press, Inc.\n\n\nBloom, B., Engelhart, M. D., Furst, E. J., Hill, W. H., & Krathwohl, D. R. (1956). Taxonomy of educational objectives: The classification of educational goals. Longman.",
    "crumbs": [
      "What is learning?",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Taxonomies of learning</span>"
    ]
  },
  {
    "objectID": "part2/interact-w-students.html#sec-cog-level-qs",
    "href": "part2/interact-w-students.html#sec-cog-level-qs",
    "title": "6  Interacting with students",
    "section": "6.5 Consider the cognitive level of your questions",
    "text": "6.5 Consider the cognitive level of your questions\nWhen asking any kind of question, we must be aware of what cognitive level our question is targeting and whether students’ responses are at that same level. (This kind of close listening is also a way of showing students that you respect what they have to say.)\nIf you ask a uni-structural question and get back a uni-structural answer, then you can be satisfied. If you ask a relational question and get back a uni-structural answer, then that’s a mismatch that you may want to, and probably should, address. Clarify with the student what kind of response you wanted from them and then try again. For example, you might say, “I was looking for a response that related these two ideas together”. Or you could prompt active listening by asking another student if the given response was at the same cognitive level as the question.\nIn fact, it’s often a good idea to tell students about the SOLO taxonomy, tell them that there are different levels of cognition that they can apply. Once students are familiar with these different cognitive levels, then you can have very fruitful metacognitive conversations. For example, you can say, “I’m about to ask you a relational question,” and the student might say, “OK, but I might only give you a uni-structural response.” A response like that is great: the student has understood what’s being asked and what cognitive level they’re at. It’s even better when the students can articulate, say, “Right now I have a multi-structural level of understanding, but I want a relational one.” We want students to be able to manage their own learning. Telling them about the SOLO taxonomy gives them useful vocabulary to serve this goal.\nAdditionally, if we tell people what level of cognition we want from them in advance, this information will change how they approach a question (Kahnemann, 2011). Even if you opt not to use the SOLO terminology, you might say something like, “I’m going to ask a question that requires you to think hard, so I’m not going to take the first answer.” Telling students up front that you expect them to think hard will make them more likely to do so.\nFinally: aim to ask students questions that are in their ballpark, that are roughly at their current level of understanding. There’s no point asking a very challenging question to someone who’s already struggling. But be aware of internalised biases in terms of whom you assume to be operating at what cognitive level (İnan-Kaya & Rubie-Davies, 2022).",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Interacting with students</span>"
    ]
  },
  {
    "objectID": "part2/interact-w-students.html#sec-build-trust",
    "href": "part2/interact-w-students.html#sec-build-trust",
    "title": "6  Interacting with students",
    "section": "6.2 Build trust",
    "text": "6.2 Build trust\nLaura thinks of trust between student and teacher in terms of “equity” in its financial sense. If equity is good—that is, if students’ trust in their teacher is good—then the teacher can challenge and push the students, and the students will go along.\nTo build good equity, to nurture a trusting student–teacher relationship, two things must be true:\n\nStudents must believe that the teacher is asking them to do something they can actually succeed at, not something impossible.\nStudents must know that the teacher will respect whatever response they give.\n\nLaura likes the financial metaphor because teacher equity can also be spent. For example, when a teacher writes a hard question or sets a difficult task, it costs them some equity, because it demands that students take a risk. Teachers must be intentional about when and how much this equity is spent.\nWithin our current assessment systems, students’ focus is on receiving a high enough number—their focus is not on deep learning. Despite this, good teacher equity can motivate students to take risks anyway, and thus to learn, because students trust that they can be successful. But the reverse can hold, too. If students aren’t trying, then that indicates that they might not trust the teacher—that there’s insufficient equity to sustain what the teacher is trying to do.",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Interacting with students</span>"
    ]
  },
  {
    "objectID": "part2/interact-w-students.html#sec-socratic",
    "href": "part2/interact-w-students.html#sec-socratic",
    "title": "6  Interacting with students",
    "section": "6.4 Socratic questioning",
    "text": "6.4 Socratic questioning\nSocratic questioning aims to prompt a student’s metacognition. Not every question directed toward a student qualifies as Socratic: “Why did you do that?” is not a Socratic question. Students often find Socratic questioning annoying, because they want answers, not more questions—but it is a very effective tool.\nA good starting point is often “Can you walk me through your thinking so far?” By retracing and articulating what they’ve done, students will often identify their own missteps. As much as possible, we want students to be the ones to notice their misunderstandings, because such metacognitive troubleshooting is a skill that will serve them in their futures.\nA good strategy for Socratic questioning is to start at the lowest cognitive level in the SOLO taxonomy (see Section 2.2). Then if the student can meet you at that level, ramp up the complexity. Specifically: Start with uni-structural questions to check whether students understand individual ideas (e.g., “Can you tell me about X?”). If they do, then progress to multi-structural and relational questions, guiding the student to connect their ideas in the target direction (e.g., “Why do you think X is happening?”, “What’s one way X is related to Y?”)",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Interacting with students</span>"
    ]
  },
  {
    "objectID": "part2/interact-w-students.html#question-with-intention",
    "href": "part2/interact-w-students.html#question-with-intention",
    "title": "6  Interacting with students",
    "section": "6.6 Question with intention",
    "text": "6.6 Question with intention\nEvery question you ask (and indeed, every task and activity you set) should have a clear intention, purpose, or goal. You should know what your question is trying to accomplish, so that you can judge whether the aim has been met.\nFor example, you might ask questions for the following reasons:\n\nAs a management strategy: to wake students up, to get them back on task, to redirect a behaviour.\nTo gauge the level of understanding in the class.\n\nBest to target these questions not toward the top students, but toward students closer to the middle of the range. Their understanding better represents the level of understanding of the class overall.\nDon’t ask “Are there any questions?”, but “At this point, what questions do you have?”\nDon’t ask “Do you understand?”, but “What’s next?” or “In this situation, what would you do?”\n\nVery broadly: to gain information that the teacher (and potentially the students) didn’t previously have.\n\nIf you ask a question without knowing what purpose it serves, then when it come to receiving an answer, you are more likely to experience confirmation bias.",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Interacting with students</span>"
    ]
  },
  {
    "objectID": "part3/designing-assessment.html#what-makes-an-assessment-good",
    "href": "part3/designing-assessment.html#what-makes-an-assessment-good",
    "title": "9  Designing an assessment",
    "section": "9.2 What makes an assessment good?",
    "text": "9.2 What makes an assessment good?\nWhat makes an assessment good depends on the goal of the assessment.\n\n9.2.1 Goal: To rank students\nIn the tradition of standardised year-end diploma exams, where much of Laura’s career has been spent, the goal of assessment has been to rank students with respect to one another. In this scenario, the goal of the test designer is to produce questions that discriminate well between students. In other words, the test should be able to tell us what “level of ability” each student has, and statistically distinguish the “good students” from the “poor students”.\nLaura notes that while this phrasing sounds like the test designers think that students’ ability levels are fixed, rather than growable/moldable (Blackwell et al., 2007; Dweck, 2006), this is not the case. This phrasing is a side effect of the way that year-end exams produce a snapshot of students’ achievement, a cross-section of their learning journey at a single point in time.\nTo check whether items on a test have done a good job discriminating between students, we have a couple options.\n\nA sophisticated way is to borrow from Item Response Theory a measure called the “corrected point bi-serial”. A bi-serial correlation asks: are students who are successful on a given item are also successful at the test overall? But being successful on a given item contributes to being successful on the test overall, and this needs to be corrected for. So, the corrected point bi-serial takes the result of the given item out of the results of the rest of the test, effectively asking: are students who are successful on a given item successful at the other items too?\nA simpler way to check discrimination is to take the five highest-scoring test papers and the five lowest-scoring test papers and to compare the nature of the responses on each item. On items that discriminate well, the responses from the top students should be different from the responses from the bottom students. If the responses of both groups of students are the same, then the item does not discriminate between them.\n\n\n\n9.2.2 Goal: To measure learning\nIn a different tradition, the goal of an assessment is to measure what a student has learned, irrespective of what anybody else in the classroom has learned. This tradition is sometimes called “mastery grading”, but this term is falling out of use because it emphasises mastering a skill when students’ focus should rather be on growth and learning, and the term also carries undesirable colonial connotations (Clark & Talbert, 2023).\nIn the context of measuring an individual student’s learning, there’s no reason to discriminate between high performers and low performers, because discrimination is about comparison. But an assessment task should still be able to identify areas of weakness and flag these to the student as areas of further growth. The test designer should have a good idea of what skills each task demands and how it would look to demonstrate those skills. We’ll elaborate on this below when we talk about “blueprinting” an assessment.\n\n\n9.2.3 The task of the test designer\nAll in all, we can summarise the task of the test designer in both traditions as follows: to design tasks that can identify successes while still detecting weaknesses.",
    "crumbs": [
      "The behavioural outcomes of learning, and how to assess them",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Designing an assessment</span>"
    ]
  },
  {
    "objectID": "part3/designing-assessment.html#blueprinting",
    "href": "part3/designing-assessment.html#blueprinting",
    "title": "9  Designing an assessment",
    "section": "9.3 Blueprinting",
    "text": "9.3 Blueprinting\nBlueprinting using a 2D matrix of content * cognitive level.\nAn assessment should not penalise a student for a particular mistake more than once.\n\n\n\n\nBlackwell, L. S., Trzesniewski, K. H., & Dweck, C. S. (2007). Implicit Theories of Intelligence Predict Achievement Across an Adolescent Transition: A Longitudinal Study and an Intervention. Child Development, 78(1), 246–263. https://doi.org/10.1111/j.1467-8624.2007.00995.x\n\n\nClark, D., & Talbert, R. (2023). Grading for growth: A guide to alternative grading practices that promote authentic learning and student engagement in higher education. Routledge.\n\n\nDweck, C. S. (2006). Mindset: The new psychology of success. Random house.",
    "crumbs": [
      "The behavioural outcomes of learning, and how to assess them",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Designing an assessment</span>"
    ]
  },
  {
    "objectID": "part2/learning-by-doing.html#some-more-things-to-think-about-in-active-learning",
    "href": "part2/learning-by-doing.html#some-more-things-to-think-about-in-active-learning",
    "title": "5  Learning happens by doing",
    "section": "5.2 Some more things to think about in active learning",
    "text": "5.2 Some more things to think about in active learning\n\nLaura has observed that attention and cognition is improved for about ten minutes following physical activity. So to engage this boost, ask students to move—for example, voting on a prompt by moving to the four corners of the room, or changing seats to work with a new partner.\nAsking students to improve an existing response is a useful activity for prompting metacognition. Improving an existing response is especially effective if the original response is from a student, rather than from a teacher (since correcting a teacher’s work could feel awkward, or if the work is already very good, students might feel that they can’t live up to that standard).\nA flipped classroom (where students prepare the material on their own time and the synchronous gatherings aim to advance students’ thinking) only works if students buy in and show initiative and if the teacher is good at presenting material clearly without an audience.\nIn a voting activity:\n\nTo model the process of learning, allow students to change their vote.\nIn-class live voting is fine for quicker thinkers, but it’s less kind to slower thinkers. To give slower thinkers time to catch up, consider including some thinking time before the vote, or modifying a Think–Pair–Share activity into Think–Pair–Vote.\n\n\n[@ Laura: What points would you like to make here about quick/slow thinking? Also, what points would you like to make about the impulsive/outspoken personality compared to the introverted/thoughtful personality? ]\n\nIn peer interaction (e.g., shoulder buddies or small informal groups):\n\nStudents learn better from one another than they learn from us (Tullis & Goldstone, 2020). But peer instruction especially improves the learning of the top students, in Laura’s experience, because the top students have the chance to explain a concept to somebody else—and explaining things deepens understanding. To make sure that struggling students still benefit from peer instruction, be transparent about how it will serve them: for example, that they’ll see the concept explained in a different way.\nWhen students are asking one another questions, listen in:\n\nWhat is the cognitive level of the questions they’re asking? (See Section 2.2.)\nWhat type of challenge is the student trying to address?\nIs their question in line with the kind of thinking you were aiming to provoke?\n\nTo interact with students while they’re interacting with their peers, Laura mostly uses prompts like a raised eyebrow or a tipped head to indicate that something more is required or that something is a bit off. To intervene more strongly, she might say something like, “Walk me through your thinking about [topic]” (see Section 6.4 on Socratic questioning).\n\nIn large group discussions:\n\nWhen one student is answering a question, other students tend to disengage, relieved that they weren’t the one who was called on. But ideally, they wouldn’t disengage—they would take the opportunity to listen and compare their understanding to their peer’s, to know whether they need to adapt. This behaviour is typically not automatic, but if teachers train students in it and model it themselves by taking student responses seriously, then the whole classroom can grow better at active listening.\nSome questions that model active learning that teachers can ask to follow up a student’s response:\n\nDo you agree with X response?\nHow would you improve X response?\nWhat else could we consider?\nWas X response better/clearer/more precise than Y response?\nWhat did Y response add to X?\nWhat else could we add?",
    "crumbs": [
      "The cognitive process of learning, and how to facilitate it",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Learning happens by doing</span>"
    ]
  },
  {
    "objectID": "part3/designing-assessment.html#three-principles-of-assessment-design",
    "href": "part3/designing-assessment.html#three-principles-of-assessment-design",
    "title": "9  Designing an assessment",
    "section": "",
    "text": "9.1.1 Fairness\nIn a fair assessment, the thing that differentiates the high performers from the low performers is their knowledge of the material and ability to extend it, rather than their personal experience or socio-economic status (SES).\nSince high ability tends to correlate with better SES (with richer families having the means to educate their children in environments with better student–teacher ratios, for example), a truly fair assessment is hard to create.\nOne way to make assessments fairer is to phrase questions and tasks plainly and clearly. Vocabulary and technical terms covered in the course can be included, but needlessly obscure words can be an obstacle. For example, Laura’s colleagues found that a question that used the word “annihilate” discriminated not between students who had learned vs. who had not learned, but between students with higher SES vs. lower SES backgrounds.\n[@ Laura: Do you have a source I can link to for the ‘annihilate’ thing?]\n\n\n9.1.2 Reliability\nIn a reliable assessment, the same construct or knowledge should be measured if the test is applied again. This is closely related to the idea in psychology of “test–retest reliability”.\n[@ Laura: What are some examples of tests that are not reliable? How do we tell in practice whether a test is/is not reliable?]\n\n\n9.1.3 Validity\nIn a valid assessment, the person doing the assessment is not surprised by the task. Under a student-centered view, which we advocate, it’s not important whether the teacher thinks the assessment is valid. What’s important is what the student thinks.\nThe test-taker should be able to say “yes, this is an appropriate way to assess [topic]”. Even if they can’t accomplish the task, it’s not the task’s fault.\nWhen we write assessments that ask students to extend known ideas, they might experience this as a threat to validity. We can be transparent about this threat by telling students that they will be extending ideas in a way that might challenge them, but that they have the tools to do so. This can help students still feel like the assessment covers tasks that are related to what they’ve learned.",
    "crumbs": [
      "The behavioural outcomes of learning, and how to assess them",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Designing an assessment</span>"
    ]
  },
  {
    "objectID": "part3/designing-assessment.html#blueprinting-an-assessment",
    "href": "part3/designing-assessment.html#blueprinting-an-assessment",
    "title": "9  Designing an assessment",
    "section": "9.3 Blueprinting an assessment",
    "text": "9.3 Blueprinting an assessment\nBlueprinting an assessment involves designing an assessment with respect to two dimensions. An assessment should test the full range of content at the full range of cognitive levels. In other words, tasks should fully populate a two-dimensional matrix that crosstabulates content with cognitive level.\n\n\n9.3.1 The dimension of content\nThe content of an assessment depends, naturally, on the course. But generally speaking, the tasks need to span the content that the students think it should span (so that the assessment is valid; see above).\nFurther, and more challengingly: an assessment should not penalise a student for a particular mistake more than once. In other words, a particular misunderstanding should not cost students marks in more than one place. Achieving this is particularly tricky when writing long tests and multiple-choice tests, because there are more opportunities for similar misconceptions to prompt students to select the wrong choices. We’ll discuss multiple choice design further in Section 11.\n\n\n9.3.2 The dimension of cognitive level\nBy “cognitive level”, we mean the kind of cognition that students need to use in order to succeed at the task. In terms of the SOLO taxonomy from Section 2.2, some examples might be:\n\nuni-structural: tasks that require memorisation.\nmulti-structural: tasks that require applying a familiar idea in a familiar way.\nrelational: tasks that require comparing/contrasting/choosing between familiar ideas.\nextended abstract: tasks that require a familiar idea to be brought into a new situation.\n\nIt is not the case that only the advanced, more demanding cognitive levels should be tested. Rather, an assessment should test all of these cognitive levels.\nSometimes, a course actually targets low-level cognition. For example, as part of Elizabeth’s undergraduate degree in linguistics, she had to memorise all of the characters of the International Phonetic Alphabet, and so she was assessed on this uni-structural knowledge. The important question to ask ourself here is: does the cognition of the questions we set match the cognition that we expect students to use?\nBut even if our course does expect students to use more sophisticated cognition, our assessments should still test the full range of cognitive levels. Why? Because every assessment should give students at every ability level a chance to succeed. By including entry-level questions, even those students who haven’t reached the relational or extended abstract cognitive levels yet can still experience some success.\nThat said, we should also be aware that cognitive level may not always correlate with overall student success.\n\nFor memorisation tasks, the cognition is easy, but success may still be low, because often students just can’t be bothered to memorise.\nFor application and comparison tasks, the cognition is middling, and in Laura’s experience, success is usually middling too. Students who struggle will usually get these wrong, while stronger students will usually get these right.\nFor synthesis tasks, where familiar ideas have to be applied in new ways, the cognition is hard, and success is consequently usually low. In Laura’s experience, success on this kind of task shows you who the hard workers are.\n\n\n\n9.3.3 Blueprinting determines the grade distribution\nChanging how questions/tasks are allocated across the two-dimensional matrix of content and cognitive level can change the distribution of grades that the assessment is likely to produce.\nConsider the following two matrices, (A) and (B), where the number in each cell shows how many questions about the respective topic target the respective cognitive level.\n(A)\n\n\n\n\n\n\n\n\n\n\n\nUni-structural\nMulti-structural\nRelational\nExtended abstract\n\n\n\n\nTopic A\n5\n3\n2\n1\n\n\nTopic B\n5\n3\n2\n1\n\n\nTopic C\n5\n3\n2\n1\n\n\n\n\n(B)\n\n\n\n\n\n\n\n\n\n\n\nUni-structural\nMulti-structural\nRelational\nExtended abstract\n\n\n\n\nTopic A\n1\n2\n3\n5\n\n\nTopic B\n1\n2\n3\n5\n\n\nTopic C\n1\n2\n3\n5\n\n\n\n\n\ncontains many more questions at lower cognitive levels than (B), so the assessment summarised in (A) will most likely produce higher grades, on the whole, than the assessment summarised in (B).\n\nThese examples are extreme, but they’re meant to illustrate the point, not to represent real or good assessments. (A) is not intrinsically better or worse than (B). What counts as a good assessment in your context depends completely on your goals. And what kind of grade distribution you target is just one of the many choices you will make when designing assessments.\n\n\n\n\nBlackwell, L. S., Trzesniewski, K. H., & Dweck, C. S. (2007). Implicit Theories of Intelligence Predict Achievement Across an Adolescent Transition: A Longitudinal Study and an Intervention. Child Development, 78(1), 246–263. https://doi.org/10.1111/j.1467-8624.2007.00995.x\n\n\nClark, D., & Talbert, R. (2023). Grading for growth: A guide to alternative grading practices that promote authentic learning and student engagement in higher education. Routledge.\n\n\nDweck, C. S. (2006). Mindset: The new psychology of success. Random house.",
    "crumbs": [
      "The behavioural outcomes of learning, and how to assess them",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Designing an assessment</span>"
    ]
  },
  {
    "objectID": "part3/designing-question.html#what-makes-a-question-good",
    "href": "part3/designing-question.html#what-makes-a-question-good",
    "title": "10  Designing individual questions",
    "section": "",
    "text": "10.1.1 Clarity\n\n\n\n10.1.2 Accuracy\n\n\n\n10.1.3 Fairness and validity\nIn Section 9, we introduced fairness and validity as two principles underlying good assessment design. These principles also apply at the level of individual questions.\n\nIf an item is fair, then it measures students’ knowledge, not correlated traits like socio-economic status.\nIf an item is valid, then students see it as a reasonable way to assess a particular topic.",
    "crumbs": [
      "The behavioural outcomes of learning, and how to assess them",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Designing individual questions</span>"
    ]
  },
  {
    "objectID": "part3/designing-question.html#choose-question-format-based-on-contentcognitive-level",
    "href": "part3/designing-question.html#choose-question-format-based-on-contentcognitive-level",
    "title": "10  Designing individual questions",
    "section": "10.2 Choose question format based on content/cognitive level",
    "text": "10.2 Choose question format based on content/cognitive level\nAsk the question in the format that makes sense for that content.",
    "crumbs": [
      "The behavioural outcomes of learning, and how to assess them",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Designing individual questions</span>"
    ]
  },
  {
    "objectID": "part3/designing-question.html#what-makes-an-item-good",
    "href": "part3/designing-question.html#what-makes-an-item-good",
    "title": "10  Designing individual questions",
    "section": "",
    "text": "10.1.1 Clarity\nIf you write a question, you know how you intend students to interpret it and what you intend them to do. But it is not always the case that students will interpret the question the way you intend.\nA good way to test your questions for clarity is to let your inner pedantic smart-aleck run wild. Here’s a recent example. For one of her statistics lectures, Elizabeth was preparing a Predict–Observe–Explain–Observe activity (see Jackson & Pankratz (2020): Tools for Teaching Science), and before presenting it to students, she ran it by Laura.\nElizabeth had written a few questions for students to think about, along with the following instructions:\n\n“Predict (individually): Write down your guesses [about the questions].\nExplain (in pairs/groups): Why did you make those predictions?\nObserve (individually): Did your guesses match the results?\nExplain (in pairs/groups): Why are the results the way they are?”\n\nLaura noted that the “Explain” step could be phrased better: “The smart-aleck response to ‘Why did you make those predictions’ is ‘Because you told me to.’”\nSo Elizabeth updated that phrasing to “Why do you think your guesses are likely to be correct?”\nIn general, tell students as explicitly as possible exactly what their task is. The challenge of a question should come from “How do I do this task?”, never from “What is the task?”\n\n\n10.1.2 Fairness and validity\nIn Section 9, we introduced fairness and validity as two principles underlying good assessment design. These principles also apply at the level of individual items.\n\nIf an item is fair, then it measures students’ knowledge, not correlated traits like socio-economic status.\nIf an item is valid, then students see it as a reasonable way to assess a particular topic.\n\n\n\n10.1.3 Accuracy\nOnce a question’s clarity, fairness, and validity are established, then the final measure for “Is a question good?” is not “Do students perform well on this question?” but “Do students who learned the material succeed at this question?” In other words, does the question accurately measure student learning by discriminating between students who learned the material and those who didn’t?\nAt the level of individual items, discriminating between students who learned and those who didn’t is no longer about ranking students. We’re not comparing students to one another, but to our idea of what they should be able to do. We want questions that can accurately detect that learning has occurred.",
    "crumbs": [
      "The behavioural outcomes of learning, and how to assess them",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Designing individual questions</span>"
    ]
  },
  {
    "objectID": "part3/designing-question.html#how-do-you-know-if-a-question-is-too-hard",
    "href": "part3/designing-question.html#how-do-you-know-if-a-question-is-too-hard",
    "title": "10  Designing individual questions",
    "section": "10.2 How do you know if a question is too hard?",
    "text": "10.2 How do you know if a question is too hard?\nQuestions might be hard because students don’t know what it’s asking them to do. So make sure that the questions are phrased clearly, as we mentioned above.\n\n\n\n\nA rule of thumb from Laura’s experience: if you show a question to an expert in the subject matter and they say “Oh, that’s a good question!”, then it’s probably too hard. The kind of nuance and subtlety that experts like will exceed the level of most students. (Laura allows one question that the experts like into each exam.)",
    "crumbs": [
      "The behavioural outcomes of learning, and how to assess them",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Designing individual questions</span>"
    ]
  },
  {
    "objectID": "part3/designing-question.html#choose-item-format-based-on-contentcognitive-level",
    "href": "part3/designing-question.html#choose-item-format-based-on-contentcognitive-level",
    "title": "10  Designing individual questions",
    "section": "10.3 Choose item format based on content/cognitive level",
    "text": "10.3 Choose item format based on content/cognitive level\nDifferent item formats (e.g., multiple choice, student-generated response) lend themselves to targeting different kinds of content and cognition, and you can capitalise on this by asking questions in the formats that make sense for your desired content/cognition.\nFor example:\n\nBecause multiple choice questions pit several responses against one another, this format is useful for detecting whether students hold particular misconceptions.\nBecause student-generated responses are more open-ended, this format is useful if you want to see whether students can do a particular task.\n\nIn the next two sections, we’ll go into much greater depth about multiple choice questions and student-generated response questions.\n\n\n\n\nJackson, S., & Pankratz, L. (2020). Tools for Teaching Science. Perimeter Institute for Theoretical Physics.",
    "crumbs": [
      "The behavioural outcomes of learning, and how to assess them",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Designing individual questions</span>"
    ]
  },
  {
    "objectID": "part3/designing-question.html#choose-item-format-based-on-targeted-contentcognitive-level",
    "href": "part3/designing-question.html#choose-item-format-based-on-targeted-contentcognitive-level",
    "title": "10  Designing individual questions",
    "section": "10.3 Choose item format based on targeted content/cognitive level",
    "text": "10.3 Choose item format based on targeted content/cognitive level\nDifferent item formats (e.g., multiple choice, student-generated response) lend themselves to targeting different kinds of content and cognition, and you can capitalise on this by asking questions in the formats that make sense for your desired content/cognition.\nFor example:\n\nBecause multiple choice questions pit several responses against one another, this format is useful for detecting whether students hold particular misconceptions.\nBecause student-generated responses are more open-ended, this format is useful if you want to see whether students can do a particular task.\n\nIn the next two sections, we’ll go into much greater depth about multiple choice questions and student-generated response questions.\n\n\n\n\nJackson, S., & Pankratz, L. (2020). Tools for Teaching Science. Perimeter Institute for Theoretical Physics.",
    "crumbs": [
      "The behavioural outcomes of learning, and how to assess them",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Designing individual questions</span>"
    ]
  },
  {
    "objectID": "part3/designing-question.html#choose-item-format-based-on-content",
    "href": "part3/designing-question.html#choose-item-format-based-on-content",
    "title": "10  Designing individual questions",
    "section": "10.3 Choose item format based on content",
    "text": "10.3 Choose item format based on content\nDifferent item formats (e.g., multiple choice, student-generated response) lend themselves to targeting different kinds of content and cognition, and you can capitalise on this by asking questions in whichever format shows you most clearly what you want to know.\nFor example:\n\nBecause multiple choice questions pit several responses against one another, this format is useful for detecting whether students hold particular misconceptions.\nBecause student-generated responses are more open-ended, this format is useful if you want to see whether students can do a particular task.\n\nIn the next two sections, we’ll go into much greater depth about multiple choice questions (Section 11) and student-generated response questions (Section 12).\n\n\n\n\nJackson, S., & Pankratz, L. (2020). Tools for Teaching Science. Perimeter Institute for Theoretical Physics.",
    "crumbs": [
      "The behavioural outcomes of learning, and how to assess them",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Designing individual questions</span>"
    ]
  },
  {
    "objectID": "part3/designing-question.html#how-do-you-know-if-an-item-is-too-hard",
    "href": "part3/designing-question.html#how-do-you-know-if-an-item-is-too-hard",
    "title": "10  Designing individual questions",
    "section": "10.2 How do you know if an item is too hard?",
    "text": "10.2 How do you know if an item is too hard?\nAn item might be hard because students don’t know what it’s asking them to do. So make sure that the item is phrased clearly, as we mentioned above.\nAn item might be hard for students who are only applying lower levels of cognition if it doesn’t offer entry points at every cognitive level. Aim to write questions where even a uni-structural response can still show that a student is thinking in the right direction.\n\n\n\n\nA rule of thumb from Laura’s experience: if you show an item to an expert in the subject matter and they say “Oh, that’s a good question!”, then it’s probably too hard. The kind of nuance and subtlety that experts like will exceed the level of most students. (Laura allows one item that the experts like into each exam.)",
    "crumbs": [
      "The behavioural outcomes of learning, and how to assess them",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Designing individual questions</span>"
    ]
  },
  {
    "objectID": "part3/designing-mcq.html#what-are-mcqs-actually-testing",
    "href": "part3/designing-mcq.html#what-are-mcqs-actually-testing",
    "title": "11  Designing multiple choice questions",
    "section": "11.2 What are MCQs actually testing?",
    "text": "11.2 What are MCQs actually testing?\nWhat MC questions are actually testing is the information that the test-taker draws on to choose between the options.",
    "crumbs": [
      "The behavioural outcomes of learning, and how to assess them",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Designing multiple choice questions</span>"
    ]
  },
  {
    "objectID": "part3/designing-mcq.html#student-behaviour-on-mcqs",
    "href": "part3/designing-mcq.html#student-behaviour-on-mcqs",
    "title": "11  Designing multiple choice questions",
    "section": "11.3 Student behaviour on MCQs",
    "text": "11.3 Student behaviour on MCQs\nThe top students should be able to read the stem, have an answer, and go hunting for that answer.",
    "crumbs": [
      "The behavioural outcomes of learning, and how to assess them",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Designing multiple choice questions</span>"
    ]
  },
  {
    "objectID": "part3/designing-mcq.html#how-many-choices-should-a-mcq-have",
    "href": "part3/designing-mcq.html#how-many-choices-should-a-mcq-have",
    "title": "11  Designing multiple choice questions",
    "section": "11.4 How many choices should a MCQ have?",
    "text": "11.4 How many choices should a MCQ have?\nAccording to Item Response Theory, an item is more informative about what a student actually knows, the more choices it has. This makes sense: if a question only has two choices, then someone guessing at random will still be right half the time. So whether they get the question right or not, we don’t know much about what they know. As the number of choices goes up, though, the probability of succeeding purely by chance goes down. So there should be as many choices as possible.\nOn the other hand, though, all but one of an item’s choices must be plausible but incorrect. And coming up with plausible incorrect choices can be a real challenge. Choices must be incorrect in different but reasonable ways, and common fillers like “all of the above” and “none of the above” aren’t actually all that useful.\n\n“All of the above” is easy for students to game: as soon as more than one choice is correct, then the correct answer must be “All of the above”, and as soon as one choice is incorrect, then the correct answer cannot be “All of the above”.\n“None of the above” doesn’t actually tell us anything about what students do know, only that they can reject a range of incorrect options.\n\nAnd in general, students can often answer MCQs correctly by reasoning strategically about the choices provided, rather than calling on any actual content knowledge.\nSo, deciding on how many choices a MCQ should contain is a matter of balancing the desire for informativeness with the challenge of crafting a large number of plausible incorrect choices.",
    "crumbs": [
      "The behavioural outcomes of learning, and how to assess them",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Designing multiple choice questions</span>"
    ]
  },
  {
    "objectID": "part3/designing-mcq.html#what-are-mcqs-testing",
    "href": "part3/designing-mcq.html#what-are-mcqs-testing",
    "title": "11  Designing multiple choice questions",
    "section": "11.2 What are MCQs testing?",
    "text": "11.2 What are MCQs testing?\nMCQs are not testing knowledge per se. As long as a student can recognise the correct answer, they don’t need to actually know it. Some people say that that counts as sufficient learning, but Laura doesn’t think so.\nWhat MCQs are really testing is what information the test-taker uses to adjudicate between the choices. But we can only see the outcome of their decision-making process. Each choice matches one way of understanding or approaching the problem (including misconceptions and ways taht are incorrect), and all we can do is infer that the student who chose Choice A is following the intended logic behind that choice. If we want to actually definitively know what a student knows, we would need to ask them to generate their response from scratch.",
    "crumbs": [
      "The behavioural outcomes of learning, and how to assess them",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Designing multiple choice questions</span>"
    ]
  },
  {
    "objectID": "part3/designing-mcq.html#behaviour-elicited-by-mcqs",
    "href": "part3/designing-mcq.html#behaviour-elicited-by-mcqs",
    "title": "11  Designing multiple choice questions",
    "section": "11.3 Behaviour elicited by MCQs",
    "text": "11.3 Behaviour elicited by MCQs\nStudents at different levels tend to behave differently when responding to a MCQ.\n\nThe bottom students will be drawn to a misconception, or they will just guess.\nThe middle students will hunt at the choices, trying to decide which one seems most plausible.\nThe top students will read the stem, have an answer, and check the choices to find that answer.\n\nIf the MCQ’s stem doesn’t elicit this behaviour in the top students, then in Laura’s experience, that item will often discriminate less well. If the top students can’t anticipate the answer, they are thrown for a loop, and so they’re less likely to succeed at the question.",
    "crumbs": [
      "The behavioural outcomes of learning, and how to assess them",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Designing multiple choice questions</span>"
    ]
  },
  {
    "objectID": "part3/designing-sgr.html#how-to-design-a-good-sgr-question",
    "href": "part3/designing-sgr.html#how-to-design-a-good-sgr-question",
    "title": "12  Designing student-generated response tasks",
    "section": "12.2 How to design a good SGR question",
    "text": "12.2 How to design a good SGR question\nThe design principles discussed previously in Section 9 and Section 11 apply here too. For example:\n\nGive entry points for all students, for all cognitive levels.\nEnsure that students know the task.\nMake sure the task is a valid ask.\n\nIn addition, it can be useful to come up with what a good answer would look like and what a weak answer would look like, and then make sure that the question really could prompt students to produce something like your target answers.",
    "crumbs": [
      "The behavioural outcomes of learning, and how to assess them",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Designing student-generated response tasks</span>"
    ]
  },
  {
    "objectID": "part3/designing-sgr.html#considerations-for-grading-sgrs",
    "href": "part3/designing-sgr.html#considerations-for-grading-sgrs",
    "title": "12  Designing student-generated response tasks",
    "section": "12.3 Considerations for grading SGRs",
    "text": "12.3 Considerations for grading SGRs\nThese considerations apply to summatively-assessed SGR questions, that is, SGRs that contribute to a student’s grade.\n\nOur standards for assessment should differ depending on whether the student’s response was produced under duress or not. “Under duress” means, for example, in a timed in-class setting, compared to a longer-term take-home project. Standards should be more lenient when students produce something under duress, and they can be stricter for a take-home project.\nStudents should be familiar with scoring criteria, rubrics, and expectations before they start generating their response and while they are doing so.\n\nIn Section 14, we’ll go into greater depth about considerations around summative assessment. But first, in Section 13, we’ll discuss the possibilities offered by assessing student work formatively.",
    "crumbs": [
      "The behavioural outcomes of learning, and how to assess them",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Designing student-generated response tasks</span>"
    ]
  },
  {
    "objectID": "part3/formative.html#peer-feedback-activity",
    "href": "part3/formative.html#peer-feedback-activity",
    "title": "13  Ideas for formative assessment",
    "section": "",
    "text": "Student A writes a response to some prompt.\nStudent B reviews A’s response and gives feedback, for example on what cognitive level (see Section 2.2) A is using.\nA responds to B’s feedback by articulating what changes they are going to make in response.\nA generates their final response.",
    "crumbs": [
      "The behavioural outcomes of learning, and how to assess them",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Ideas for formative assessment</span>"
    ]
  },
  {
    "objectID": "part3/formative.html#multiple-choice-with-certainty-ratings",
    "href": "part3/formative.html#multiple-choice-with-certainty-ratings",
    "title": "13  Ideas for formative assessment",
    "section": "13.2 Multiple choice with certainty ratings",
    "text": "13.2 Multiple choice with certainty ratings\nGive students a series of multiple choice questions (MCQs), but instead of having them select one answer, have them assign percentages to each of the choices, based on how confident they are that that choice is the correct answer. This task triggers students’ metacognition because it asks them to reflect: did I know this, or was I uncertain?\nIf students were very confident about a wrong answer, then this is good news! It highlights an area where we can do work to improve the student’s understanding.\nThis activity scales well, too: it can easily be implemented in a web-based forms service like Google or Microsoft Forms.",
    "crumbs": [
      "The behavioural outcomes of learning, and how to assess them",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Ideas for formative assessment</span>"
    ]
  },
  {
    "objectID": "part3/formative.html#formative-responses-to-student-generated-work",
    "href": "part3/formative.html#formative-responses-to-student-generated-work",
    "title": "13  Ideas for formative assessment",
    "section": "13.3 Formative responses to student-generated work",
    "text": "13.3 Formative responses to student-generated work\nIt’s often laborious to respond with detailed feedback on student-generated written work. An alternative that Laura uses is to colour in the margins of the work with three different colours of pen or highlighter. Each colour means something different about that section of the text.\n\nColour 1: Things are good.\nColour 2: Things are going off track.\nColour 3: Things are off track.\n\nThis method of broad-strokes feedback places the onus on the students to figure out what exactly in their text each colour applies to, and to identify what exactly needs fixing. It helps students become reflective and critical of their own work. (It also shows visually who was successful, so that students can see which of their peers they might ask for help.)",
    "crumbs": [
      "The behavioural outcomes of learning, and how to assess them",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Ideas for formative assessment</span>"
    ]
  },
  {
    "objectID": "part3/summative.html#things-to-think-about-when-designing-scoring-rubrics",
    "href": "part3/summative.html#things-to-think-about-when-designing-scoring-rubrics",
    "title": "14  Considerations around summative assessment",
    "section": "14.2 Things to think about when designing scoring rubrics",
    "text": "14.2 Things to think about when designing scoring rubrics\nTeachers often arrive at final grades for student work by using scoring rubrics: two-dimensional matrices that map certain levels of achievement on certain skills to a numeric grade. But in Laura’s experience, most rubrics could be improved by more thoughtful design and sense-checking. That’s what we’ll focus on in this final section.\n\n14.2.1 How many outcome categories?\nBy “outcome categories”, we mean the scale of levels that students’ responses can achieve (an example from rubrics used in Edinburgh’s Psychology department: “developing”, “proficient”, and “excellent”). Much like changing an assessment blueprint can change the grade distribution (see Section 9), changing the number of outcome categories in a rubric can do the same.\nTwo examples from Grade 12 courses in Alberta:\n\nThe general science course grades student-generated responses using a four-point scoring rubric.\n\n1 is failure.\n2, 3, and 4 are degrees of success.\n\nThe physics and chemistry courses grades student-generated responses using a five-point scoring rubric.\n\n1 and 2 are degrees of failure.\n3, 4, and 5 are degrees of success.\n\n\nBoth rubrics have three successful outcome categories. But because the physics/chemistry rubric has two failure categories, not just one, students have to produce a better response in physics/chemistry than they would have to in the general science course to avoid a failing grade. In other words, because of the rubric design, passing the general science course is harder than passing the physics/chemistry courses. This is intentional: the goal of the general science course is for students to be able to pass. It was designed to be less challenging than the other, more specialised courses, and rubrics offer a mechanism for implementing this decision.\n\nThere’s no wrong answer here, with respect to the number of categories. Be intentional, and choose what works for your context.\n\n\n14.2.2 How to label outcome categories\nIn the example from Psychology at the University of Edinburgh, the outcome categories are labelled “developing”, “proficient”, and “excellent”. And in the examples from Alberta high school courses, the outcome categories are labelled numerically: 1 through 4 or 1 through 5. Whether to use verbal descriptions or numbers to label the categories varies by discipline, and there is no right or wrong here.\nIf verbal descriptions are used, then they should focus on what qualities of the response are present, not what are missing. This principle applies not just to labelling outcome categories, but also to any verbal description of achievement at each outcome level, as we’ll discuss next.\n\n\n14.2.3 How to describe achievement at each level\nRubrics should always phrase descriptions in terms of what qualities of the response are present.\n[@ Laura: Do you have a before/after example of how to rephrase a “quality is absent” kind of description?]\nFraming a rubric in terms of what is present, not what’s missing, has a number of benefits:\n\nIt tells students more clearly what their response needs to do.\nIt lets us apply the rubric to a wider range of non-traditional assessment formats, such as unessays.\n\nDescribing different levels of achievement only in terms of what is present is a challenge. One way to achieve it is to focus on the nature of the response and the nature of the errors, incorporating the SOLO taxonomy (Section 2.2). For instance, imagine the teacher has set a multi-structural task, and received two different kinds of responses:\n\nOne response contains uni-structural answers.\nOne response contains multi-structural answers with a few mistakes.\n\nA rubric that ranks responses by their cognitive level is able to give the multi-structural response the higher score it deserves.\n\n\n14.2.4 How to sense-check a rubric\nOnce you have developed a working rubric, test it by letting your inner smart-aleck apply it to a few sample documents.\nThe first document should be a blank sheet of paper. It may sound silly, but consider a rubric with statements like:\n\n“Response does not address the question.” Check! A blank page doesn’t address the question.\n“Response contains no errors.” Check! A blank page contains no errors.\n\nBy this imaginary rubric, the blank page will score extremely high for factualness and extremely low for relevance. But it’s a blank page. A good rubric should not allow a blank page to pass.\nThe second document to test should be an irrelevant statement of truth. Laura’s go-to example: “I like cheese.” Again:\n\n“Response does not address the question.” Check! This statement is irrelevant.\n“Response contains no errors.” Check! Laura does like cheese.\n\nA good rubric should not allow irrelevant true responses to succeed.\nAfter these basic tests, subsequent documents can be pieces of student work for which you have a sense of the outcome you want them to achieve. Do they perform the way you think they should?\nTweak, iterate, and repeat until you have positively-phrased rubrics that reflect your desired pass rate.\n\n\n\n\nButler, R., & Nisan, M. (1986). Effects of no feedback, task-related comments, and grades on intrinsic motivation and performance. Journal of Educational Psychology, 78(3), 210.",
    "crumbs": [
      "The behavioural outcomes of learning, and how to assess them",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Considerations around summative assessment</span>"
    ]
  },
  {
    "objectID": "part3/summative.html#dichotomous-vs.-partial-marks",
    "href": "part3/summative.html#dichotomous-vs.-partial-marks",
    "title": "14  Considerations around summative assessment",
    "section": "",
    "text": "Studies run by Alberta Education found that allowing partial marks on multiple choice questions improves discrimination between higher performers and lower performers. [@ Laura: Is there anything we can cite for this result?]\nTo implement partial marks for multiple choice questions, the test designer has to decide how much a particular misconception (assumed to be indicated by the student selecting a particular choice) should penalise the student, and they must communicate this to the students.\nStudents’ behaviour changes when they know partial marks are involved, compared to when they know the test uses dichotomous marks. That their behaviour changes means that tests do not only measure students’ content knowledge, but also their test-taking savvy and strategy.",
    "crumbs": [
      "The behavioural outcomes of learning, and how to assess them",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Considerations around summative assessment</span>"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "In one of our first “business meetings” about this little book, I (Elizabeth) asked Laura to think about a title. A title is a big decision, and I expected the deliberation to take a while. But within seconds, Laura said, “What about something like … ‘Learning is messy, performance is beautiful’?” How succinctly that parallel summed up everything we’d been discussing!\nLearning is messy. A foundation of Laura’s philosophy. Foundations are shaky, ideas are misunderstood—and that is all part of the process. Within the mess, an excellent teacher can always find something good, something worth building on. And what’s not worth building on is not a “mistake”, but a pointer toward some misunderstanding that an excellent teacher can help improve.\nI’ve embraced this idea wholeheartedly too. But for the second part of the title, I did put my own stink on things. I don’t view performance as especially beautiful. Performance is what our modern educational systems force students to do, so that we can (inaccurately) summarise their achievement (not their learning) with a single number (which employers tend not to even care about as much as educators think they do). “Beautiful” is too nice a word for so uncaring a thing.\nBut performance is polished. Speaking as somebody who was a student herself not too long ago: students are very practiced at playing the game of grades. It’s easier to pass an exam by stuffing one’s short-term memory full of facts than by engaging in the messy work of genuine, long-term, sustained learning—so that’s what students do, myself absolutely included. This behaviour is essentially self-preservation, and it’s emerged because along the way, educators’ well-intentioned goal of understanding what students have learned has mutated into an ungenerous demand for a polished song-and-dance.\nBut students adapt to how we assess. And if we assess fairly and with a focus on real learning (not to mention on students’ well-being), for example with more thoughtful assessment design, emphasis on formative assessment, and even adoption of alternative assessment methods (Blum, 2020; Clark & Talbert, 2023; Sambell et al., 2012), then I do think that would be a beautiful thing.\n\n\n\nFor my whole life, I’ve looked up to Laura as a teacher. She helped me through many classes I wasn’t sure I’d pass. Now, a teacher myself, I’m really happy to have had this chance to collect and share her wisdom and experience. We both hope that you, reader, will gain something from it that will help you guide your students through the messy process of learning.\n\n\n\n\nBlum, S. D. (Ed.). (2020). Ungrading: Why rating students undermines learning (and what to do instead). West Virginia University Press.\n\n\nClark, D., & Talbert, R. (2023). Grading for growth: A guide to alternative grading practices that promote authentic learning and student engagement in higher education. Routledge.\n\n\nSambell, K., McDowell, L., & Montgomery, C. (2012). Assessment for learning in higher education. Routledge.",
    "crumbs": [
      "Conclusion"
    ]
  }
]